# -*- coding: utf-8 -*-
"""capstone_full_model_sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pMK6N4Fr5DtL0NokYFAt9263g3evZSDG
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Libraries"""

from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Flatten, Subtract
from tensorflow.keras.models import Model
import numpy as np
import matplotlib.pyplot as plt
import os
import tensorflow as tf
from tensorflow import keras

"""# Functions"""

autoencoder_weight_path = "/content/drive/MyDrive/Colab/training_checkpoint/training_autoencoder/cp.ckpt"

def create_autoencoder():
  input = Input(shape=(28, 28, 1))
  encoded = Conv2D(16, (3,3), activation = 'relu', padding = 'same')(input)
  encoded = Conv2D(16, (3,3), activation = 'relu', padding = 'same')(encoded)
  encoded = MaxPooling2D((2,2), padding='same')(encoded)
  encoded = Conv2D(8, (3,3), activation = 'relu', padding = 'same')(encoded)
  encoded = Conv2D(8, (3,3), activation = 'relu', padding = 'same')(encoded)
  encoded = MaxPooling2D((2,2), padding='same')(encoded)

  decoded = Conv2D(8, (3,3), activation='relu', padding='same')(encoded)
  decoded = Conv2D(8, (3,3), activation='relu', padding='same')(decoded)
  decoded = UpSampling2D((2,2))(decoded)
  decoded = Conv2D(16, (3,3), activation='relu', padding='same')(decoded)
  decoded = Conv2D(16, (3,3), activation='relu', padding='same')(decoded)
  decoded = UpSampling2D((2,2))(decoded)
  output = Conv2D(1, (3,3), activation='sigmoid', padding='same')(decoded)

  model = Model(inputs = [input], outputs = [output])
  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

  #model.load_weights(autoencoder_weight_path)
  return model

def get_encoder():
  #need some change to load full model!
  model_autoencoder = create_autoencoder()
  model_autoencoder.load_weights(autoencoder_weight_path)
  encoder_layers = [layer.name for layer in model_autoencoder.layers]
  encoder_layers = encoder_layers[int(len(encoder_layers)/2)]
  #print(encoder_layers)
  
  #encoder_model = Model(inputs=model_autoencoder.input, outputs=model_autoencoder.output)
  encoder_model = Model(inputs=model_autoencoder.input, outputs=model_autoencoder.get_layer(encoder_layers).output)
  return encoder_model

def get_decoder():
  #need some change to load full model!
  model_autoencoder = create_autoencoder()
  model_autoencoder.load_weights(autoencoder_weight_path)
  decoder_layers = [layer.name for layer in model_autoencoder.layers]
  decoder_layers = decoder_layers[int(len(decoder_layers)/2)]
  #print(encoder_layers)
  
  #encoder_model = Model(inputs=model_autoencoder.input, outputs=model_autoencoder.output)
  decoder_model = Model(inputs=model_autoencoder.get_layer(decoder_layers).output, outputs=model_autoencoder.output)
  return decoder_model

def show_images(numb, images, arr_on=True):
  if arr_on:
    if numb <= 20:
      number = 1
    else:
      number = int(numb/20)
    for o in range(number):
      if o == number-1:
        n = numb%20
        if n == 0:
          n = 20
      else:
        n = 20
      for now in images:
        plt.figure(figsize=(20, 4))
        for i in range(n*o, n*(o+1)):
          # plt.subplot( nrow, ncol, index)
          ax = plt.subplot(2, n, i+1)
          plt.imshow(now[i].reshape(28, 28))
          plt.gray()
          ax.get_xaxis().set_visible(False)
          ax.get_yaxis().set_visible(False)
          #print(np.sum(np.abs(decoded_imgs[i] - x_test[i])))t.gray()
        plt.show()
  else:
    plt.figure(figsize=(20, 4))
    # plt.subplot( nrow, ncol, index)
    ax = plt.subplot(2, 1, 1)
    plt.imshow(images.reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    #print(np.sum(np.abs(decoded_imgs[i] - x_test[i])))t.gray()
    plt.show()

def get_data_comparison(datas, labels):
  #gather datas - same number
  gather_images = []
  for i in range(10):
    gather = []
    for j in range(len(labels)):
      if(labels[j] == i):
        gather.append(datas[j])
    gather_images.append(np.array(gather))

  #data split - same(half split), diffrent(full)
  same_images_1 = []
  same_images_2 = []
  diffrent_images = []

  for now in gather_images:
    data_same = now[:int(len(now)/2)+1]
    data_diffrent = now[int(len(now)/2)+1:]
    for num in range(0, len(data_same)-1,2):
      same_images_1.append(data_same[num])
      same_images_2.append(data_same[num+1])
    diffrent_images.append(data_diffrent)

  #diffrent data split
  diffrent_images_1 = []
  diffrent_images_2 = []

  while(True):
    length = len(diffrent_images)
    for i in range(length):
      for j in range(length):
        if(i != j):
          if(len(diffrent_images[i]) != 0 and len(diffrent_images[j]) != 0):
            origin = diffrent_images[i]
            control = diffrent_images[j]
            diffrent_images_1.append(origin[0])
            diffrent_images_2.append(control[0])
            diffrent_images[i] = origin[1:]
            diffrent_images[j] = control[1:]
    zero = []
    for i in range(length):
      if len(diffrent_images[i]) == 0:
        zero.append(i)
    zero.sort(reverse=True)
    for i in zero:
      del diffrent_images[i]
    if len(diffrent_images) <= 1:
      break

  #shuffle datas and write labels
  origin_input_images = []
  control_input_images = []
  entire_labels = []

  while(True):
    same, diffrent = np.random.randint(6, size=2)
    if same > len(same_images_1):
      same = len(same_images_1)
    for i in range(same):
      origin_input_images.append(same_images_1[i])
      control_input_images.append(same_images_2[i])
      entire_labels.append(1)
    same_images_1 = same_images_1[same:]
    same_images_2 = same_images_2[same:]
    if diffrent > len(diffrent_images_1):
      diffrent = len(diffrent_images_1)
    for i in range(diffrent):
      origin_input_images.append(diffrent_images_1[i])
      control_input_images.append(diffrent_images_2[i])
      entire_labels.append(0)
    diffrent_images_1 = diffrent_images_1[diffrent:]
    diffrent_images_2 = diffrent_images_2[diffrent:]
    if len(same_images_1) == 0 and len(diffrent_images_1) == 0:
      break
  
  return [np.array(origin_input_images), np.array(control_input_images), np.array(entire_labels)]

def get_latent(input_data):
  encoder = get_encoder()
  if len(input_data) == 1:
    return encoder.predict(input_data)
  result = []
  for data in input_data:
    result.append(encoder.predict(data))
  return result

"""# 데이터 - MNIST

테스트용 데이터 : mnist
"""

from tensorflow.keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.astype('float32') / 255.
test_images  = test_images.astype('float32') / 255.

train_images = train_images.reshape((len(train_images), 28, 28, 1))
test_images = test_images.reshape((len(test_images), 28, 28, 1))

print(train_images.shape)
print(train_labels.shape)
print(test_images.shape)
print(test_labels.shape)

noise_factor = 0.75

train_noise_images = train_images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_images.shape)
test_noise_images = test_images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=test_images.shape)

train_noise_images = np.clip(train_noise_images, 0., 1.)
test_noise_images = np.clip(test_noise_images, 0., 1.)

'''
n = 10

plt.figure(figsize=(20,4))

for i in range(n):
  ax = plt.subplot(2, n, i+1)
  plt.imshow(train_noise_images[i].reshape(28, 28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

  ax = plt.subplot(2, n, i+1+n)
  plt.imshow(train_images[i].reshape(28, 28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

plt.show()
'''
show_images(10, [train_noise_images, train_images])

print(train_noise_images.shape)
print(train_labels.shape)
print(test_noise_images.shape)
print(test_labels.shape)

"""# autoencoder 모의 구현


"""

autoencoder = create_autoencoder()

#from keras.utils import plot_model
#plot_model(autoencoder, show_shapes=True)

'''
checkpoint_path = "/content/drive/MyDrive/Colab/training_checkpoint/training_autoencoder/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)
'''

#autoencoder.load_weights(checkpoint_path)

autoencoder.fit(train_noise_images, train_images, epochs=10, batch_size=256, shuffle=True, validation_data=(test_images, test_images), callbacks=[cp_callback])

'''
autoencoder = create_autoencoder()
loss, acc = autoencoder.evaluate(test_noise_images, test_images, verbose=2)
print("Untrained model, accuracy: {:5.2f}%".format(100 * acc))

autoencoder.load_weights(checkpoint_path)
loss, acc = autoencoder.evaluate(test_noise_images, test_images, verbose=2)
print("Restored model, accuracy: {:5.2f}%".format(100 * acc))
'''

#os.listdir(checkpoint_dir)

decoded_images = autoencoder.predict(test_noise_images)

show_images(10, [test_noise_images, decoded_images])

"""# encoder/decoder 추출"""

autoencoder.summary()

print('Layers : ', autoencoder.layers)

layer_names = [layer.name for layer in autoencoder.layers]
print('Layer names : ', layer_names)

activation_model = Model(inputs=autoencoder.input, outputs=autoencoder.output)

print(type(test_images), type(test_images[0]))

activations = autoencoder.predict(test_noise_images)
print(activations.shape)

activations = activation_model.predict(np.array([test_noise_images[0]]))
print(activations.shape)

'''
plt.figure(figsize=(20, 4))

ax = plt.subplot(2, 2, 1)
plt.imshow(test_noise_images[0].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)

ax = plt.subplot(2, 2, 3)
plt.imshow(activations.reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
'''
show_images(1, [test_noise_images[0], activations], False)

'''
def get_encoder():
  #need some change to load full model!
  model_autoencoder = create_autoencoder()
  model_weight_path = "/content/drive/MyDrive/Colab/training_checkpoint/training_autoencoder/cp.ckpt"
  model_autoencoder.load_weights(model_weight_path)
  encoder_layers = [layer.name for layer in model_autoencoder.layers]
  encoder_layers = encoder_layers[int(len(encoder_layers)/2)]
  #print(encoder_layers)
  
  #encoder_model = Model(inputs=model_autoencoder.input, outputs=model_autoencoder.output)
  encoder_model = Model(inputs=model_autoencoder.input, outputs=model_autoencoder.get_layer(encoder_layers).output)
  return encoder_model
'''

encoder = get_encoder()

latent_value = encoder.predict(np.array([test_noise_images[0], test_noise_images[1]]))

latent_value.shape

"""decoder 추출"""

'''
def get_decoder():
  #need some change to load full model!
  model_autoencoder = create_autoencoder()
  model_weight_path = "/content/drive/MyDrive/Colab/training_checkpoint/training_autoencoder/cp.ckpt"
  model_autoencoder.load_weights(model_weight_path)
  decoder_layers = [layer.name for layer in model_autoencoder.layers]
  decoder_layers = decoder_layers[int(len(decoder_layers)/2)]
  #print(encoder_layers)
  
  #encoder_model = Model(inputs=model_autoencoder.input, outputs=model_autoencoder.output)
  decoder_model = Model(inputs=model_autoencoder.get_layer(decoder_layers).output, outputs=model_autoencoder.output)
  return decoder_model
'''

decoder = get_decoder()

final_decoder = decoder.predict(latent_value)

final_decoder.shape

'''
n = 2

#그 중 10장을 선택하여 가시화 (입력 10장, 복원된 출력 10장)
plt.figure(figsize=(20, 4))

for i in range(n):
    # plt.subplot( nrow, ncol, index)
    ax = plt.subplot(2, n, i+1)
    plt.imshow(test_noise_images[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    ax = plt.subplot(2, n, i+1+n)
    plt.imshow(final_decoder[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    #print(np.sum(np.abs(decoded_imgs[i] - x_test[i])))

plt.show()
'''
show_images(2, [test_noise_images, final_decoder])

"""# latent - 시각화

# 데이터 - 대조군 비교
"""

#test - origin data
test_model = create_autoencoder()

test = test_model.predict(test_images)

show_images(20, [test_images, test])

#make data for contrast - just use mnist(need to change)
'''
def get_data_comparison(datas, labels):
  #gather datas - same number
  gather_images = []
  for i in range(10):
    gather = []
    for j in range(len(labels)):
      if(labels[j] == i):
        gather.append(datas[j])
    gather_images.append(np.array(gather))

  #data split - same(half split), diffrent(full)
  same_images_1 = []
  same_images_2 = []
  diffrent_images = []

  for now in gather_images:
    data_same = now[:int(len(now)/2)+1]
    data_diffrent = now[int(len(now)/2)+1:]
    for num in range(0, len(data_same)-1,2):
      same_images_1.append(data_same[num])
      same_images_2.append(data_same[num+1])
    diffrent_images.append(data_diffrent)

  #diffrent data split
  diffrent_images_1 = []
  diffrent_images_2 = []

  while(True):
    length = len(diffrent_images)
    for i in range(length):
      for j in range(length):
        if(i != j):
          if(len(diffrent_images[i]) != 0 and len(diffrent_images[j]) != 0):
            origin = diffrent_images[i]
            control = diffrent_images[j]
            diffrent_images_1.append(origin[0])
            diffrent_images_2.append(control[0])
            diffrent_images[i] = origin[1:]
            diffrent_images[j] = control[1:]
    zero = []
    for i in range(length):
      if len(diffrent_images[i]) == 0:
        zero.append(i)
    zero.sort(reverse=True)
    for i in zero:
      del diffrent_images[i]
    if len(diffrent_images) <= 1:
      break

  #shuffle datas and write labels
  origin_input_images = []
  control_input_images = []
  entire_labels = []

  while(True):
    same, diffrent = np.random.randint(6, size=2)
    if same > len(same_images_1):
      same = len(same_images_1)
    for i in range(same):
      origin_input_images.append(same_images_1[i])
      control_input_images.append(same_images_2[i])
      entire_labels.append(1)
    same_images_1 = same_images_1[same:]
    same_images_2 = same_images_2[same:]
    if diffrent > len(diffrent_images_1):
      diffrent = len(diffrent_images_1)
    for i in range(diffrent):
      origin_input_images.append(diffrent_images_1[i])
      control_input_images.append(diffrent_images_2[i])
      entire_labels.append(0)
    diffrent_images_1 = diffrent_images_1[diffrent:]
    diffrent_images_2 = diffrent_images_2[diffrent:]
    if len(same_images_1) == 0 and len(diffrent_images_1) == 0:
      break
  
  return [np.array(origin_input_images), np.array(control_input_images), np.array(entire_labels)]
'''

origin_data, control_data, comparison_labels = get_data_comparison(train_images, train_labels)
origin_data.shape

show_images(40, [origin_data, control_data])
print(comparison_labels[:20])
print(comparison_labels[20:40])

'''
def get_latent(input_data):
  encoder = get_encoder()
  if len(input_data) == 1:
    return encoder.predict(input_data)
  result = []
  for data in input_data:
    result.append(encoder.predict(data))
  return result
'''

latent_origin, latent_control = get_latent([origin_data, control_data])

latent_origin.shape

"""# 대조군 비교 모델 - 미완성"""

decoder = get_decoder()

train_full = (train_control_latent - train_origin_latent) * train_origin_latent
print(train_full.shape)

result = decoder.predict(train_full)

show_images(20, [result, train_origin_data, train_control_data])
print(train_comparison_labels[:10])

def create_comparison():
  input_origin = Input(shape=(7, 7, 8))
  input_control = Input(shape=(7, 7, 8))

  sub = Subtract()([input_origin, input_control])
  pool = MaxPooling2D((2, 2), padding='same')(sub)

  flatten = Flatten()(pool)

  output = Dense(1, activation='sigmoid')(flatten)


  model = Model(inputs=[input_origin, input_control], outputs=[output])
  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

  return model

comparison_model = create_comparison()

comparison_model.summary()

train_origin_data, train_control_data, train_comparison_labels = get_data_comparison(train_images, train_labels)

train_origin_latent, train_control_latent = get_latent([origin_data, control_data])

test_origin, test_control, test_comparison_labels = get_data_comparison(test_images, test_labels)

test_origin_latent, test_control_latent = get_latent([test_origin, test_control])

comparison_model.fit([train_origin_latent, train_control_latent], train_comparison_labels, epochs=20, batch_size=256, shuffle=True, validation_data=([test_origin_latent, test_control_latent], test_comparison_labels))

loss, acc = comparison_model.evaluate([test_origin_latent, test_control_latent], test_comparison_labels, verbose=2)
print("Untrained model, accuracy: {:5.2f}%".format(100 * acc))

result = comparison_model.predict([train_control_latent, train_origin_latent])
for i in range(len(result)):
  result[i] = result[i] * 10
  if result[i] > 5:
    result[i] = 1
  else:
    result[i] = 0
count = 0
for i in range(len(train_comparison_labels)):
  if train_comparison_labels[i] != result[i]:
    count += 1
final_count = np.round(count/len(train_comparison_labels), 2)
print(count, '/', len(train_comparison_labels), '=', final_count)
print(train_comparison_labels[:10])
print(result[:10])

origin = test_origin_latent[10]
control = test_control_latent[456]

print(origin.shape)
print(control.shape)

result = comparison_model.predict([test_origin_latent, test_control_latent])
for i in range(len(result)):
  result[i] = result[i] * 10
  if result[i] > 5:
    result[i] = 1
  else:
    result[i] = 0

count = 0
for i in range(len(test_comparison_labels)):
  if test_comparison_labels[i] != result[i]:
    count += 1
final_count = np.round(count/len(test_comparison_labels), 2)
print(count, '/', len(test_comparison_labels), '=', final_count)
print(test_comparison_labels[:10])
print(result[:10])

"""# Contrastive Learning - 대조군 비교"""

(train_images_contra, train_labels_contra), (test_images_contra, test_labels_contra) = mnist.load_data()

train_images_contra = train_images_contra.astype('float32') / 255.
test_images_contra  = test_images_contra.astype('float32') / 255.

train_images_contra = train_images_contra.reshape(-1, 28, 28, 1)
test_images_contra = test_images_contra.reshape(-1, 28, 28, 1)

print(train_images_contra.shape)
print(train_labels_contra.shape)
print(test_images_contra.shape)
print(test_labels_contra.shape)

from PIL import Image

def data_augmentation_contra(images):
  image = images.copy()
  if image.shape == (28, 28, 1):
    x_start = np.random.randint(20)
    y_start = np.random.randint(20)

    # 해당 부분 회색 마킹
    image[x_start:x_start+9, y_start:y_start+9] = 0.6
    # 마지막 두 axis 기준 90도 회전
    image = np.rot90(image, 1)
  else:
    for i in range(len(image)):
      now = image[i]

      x_start = np.random.randint(20)
      y_start = np.random.randint(20)

      # 해당 부분 회색 마킹
      now[x_start:x_start+9, y_start:y_start+9] = 0.6
      # 마지막 두 axis 기준 90도 회전
      image[i] = np.rot90(now, 1)
  return image

from matplotlib.pyplot import style

style.use('default')
figure = plt.figure()
figure.set_size_inches(4, 2)

# 흑백으로 출력하기 위한 스타일 설정
style.use('grayscale')

# 1 * 2 사이즈의 격자 설정
axes = []
for i in range(1, 3):
    axes.append(figure.add_subplot(1, 2, i))

# 첫 이미지에 대한 원본 이미지 및 augmentation 수행된 이미지 시각화
img_example = train_images_contra
original = img_example[0].reshape(-1, 28)
aug_img = data_augmentation_contra(img_example)[0].reshape(-1, 28)

axes[0].matshow(original)
axes[1].matshow(aug_img)

# 제목 설정 및 눈금 제거
axes[0].set_axis_off()
axes[0].set_title('original')
axes[1].set_axis_off() 
axes[1].set_title('augmentation')

plt.show()

import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1)
        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1)
        self.fc = nn.Linear(4 * 4 * 20, 100)

    def forward(self, x):
        x = F.relu(self.conv1(x)) # (batch, 1, 28, 28) -> (batch, 10, 24, 24)

        x = F.max_pool2d(x, kernel_size=2, stride=2) # (batch, 10, 24, 24) -> (batch, 10, 12, 12)

        x = F.relu(self.conv2(x)) # (batch, 10, 12, 12) -> (batch, 20, 8, 8)

        x = F.max_pool2d(x, kernel_size=2, stride=2) # (batch, 20, 8, 8) -> (batch, 20, 4, 4)

        x = x.view(-1, 4 * 4 * 20) # (batch, 20, 4, 4) -> (batch, 320)

        x = F.relu(self.fc(x)) # (batch, 320) -> (batch, 100)
        return x # (batch, 100)

class SimCLR_Loss(nn.Module):
    def __init__(self, batch_size, temperature):
        super().__init__()
        self.batch_size = batch_size
        self.temperature = temperature

        self.mask = self.mask_correlated_samples(batch_size)
        self.criterion = nn.CrossEntropyLoss(reduction="sum")
        self.similarity_f = nn.CosineSimilarity(dim=2)

    # loss 분모 부분의 negative sample 간의 내적 합만을 가져오기 위한 마스킹 행렬
    def mask_correlated_samples(self, batch_size):
        N = 2 * batch_size
        mask = torch.ones((N, N), dtype=bool)
        mask = mask.fill_diagonal_(0)
        
        for i in range(batch_size):
            mask[i, batch_size + i] = 0
            mask[batch_size + i, i] = 0
        return mask

    def forward(self, z_i, z_j):

        N = 2 * self.batch_size

        z = torch.cat((z_i, z_j), dim=0)

        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature

        # loss 분자 부분의 원본 - augmentation 이미지 간의 내적 합을 가져오기 위한 부분
        sim_i_j = torch.diag(sim, self.batch_size)
        sim_j_i = torch.diag(sim, -self.batch_size)
        
        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)
        negative_samples = sim[self.mask].reshape(N, -1)
        
        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long()
        
        logits = torch.cat((positive_samples, negative_samples), dim=1)
        loss = self.criterion(logits, labels)
        loss /= N
        
        return loss

device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')

X_train_aug = data_augmentation_contra(train_images_contra) # 각 X_train 데이터에 대하여 augmentation

train_contra = train_images_contra.reshape(-1, 1, 28, 28)
X_train_aug = X_train_aug.reshape(-1, 1, 28, 28)

train_contra = torch.tensor(train_contra)
X_train_aug = torch.tensor(X_train_aug)

print(train_contra.shape)
print(X_train_aug.shape)

from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
from tqdm.notebook import tqdm
import torch

X_train_aug = X_train_aug.to(device) # 학습을 위하여 GPU에 선언

dataset = TensorDataset(train_contra, X_train_aug) # augmentation된 데이터와 pair
batch_size = 32

dataloader = DataLoader(dataset, batch_size = batch_size)

model = CNN() # 모델 변수 선언
loss_func = SimCLR_Loss(batch_size, temperature = 0.5) # loss 함수 선언

# train 코드 예시
epochs = 10
model.to(device)
model.train()

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

for i in range(1, epochs + 1):
    total_loss = 0
    for data in tqdm(dataloader):
        origin_vec = model(data[0])
        aug_vec = model(data[1])

        loss = loss_func(origin_vec, aug_vec)
        total_loss += loss.item()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print('Epoch : %d, Avg Loss : %.4f'%(i, total_loss / len(dataloader)))

class CNN_classifier(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.CNN = model # contrastive learning으로 학습해둔 모델을 불러오기
        self.mlp = nn.Linear(100, 10) # class 차원 개수로 projection

    def forward(self, x):
        x = self.CNN(x) # (batch, 100)으로 변환
        x = self.mlp(x) # (batch, 10)으로 변환
        return x # (batch, 10)

    def predict(self, x):
      origin_x = x[0]
      comparison_x = x[1]
      origin = self.CNN(origin_x)
      comparison = self.CNN(comparison_x)
      result = 0
      return result

X_train = torch.tensor(train_images_contra)
Y_train = torch.tensor(train_labels_contra)

X_train = X_train.reshape(-1, 1, 28, 28)

print(X_train.shape)
print(Y_train.shape)

class_dataset = TensorDataset(X_train, Y_train) # 데이터와 라벨 간의 pair
batch_size = 32

class_dataloader = DataLoader(class_dataset, batch_size = batch_size)

classifier = CNN_classifier(model).to(device) # 모델 선언, GPU 활용 지정

classifier_loss = nn.CrossEntropyLoss() # 분류를 위한 loss 함수

epochs = 10
classifier.train()

optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)

for i in range(1, epochs + 1):
    correct = 0
    for data in tqdm(class_dataloader):
        logits = classifier(data[0])

        loss = classifier_loss(logits, data[1].long())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        correct += torch.sum(torch.argmax(logits, 1) == data[1]).item() # 정확도 산출을 위하여 정답 개수 누적

    print('Epoch : %d, Train Accuracy : %.2f%%'%(i, correct * 100 / len(X_train)))

X_test = torch.tensor(test_images_contra)
Y_test = torch.tensor(test_labels_contra)

X_test = X_test.reshape(-1, 1, 28, 28)

print(X_test.shape)

test_dataset = TensorDataset(X_test, Y_test) # 테스트 데이터와 라벨 pair
batch_size = 32

test_dataloader = DataLoader(test_dataset, batch_size = batch_size)

classifier.eval() # 테스트 모드로 전환

correct = 0
for data in tqdm(test_dataloader):

    logits = classifier(data[0])
    correct += torch.sum(torch.argmax(logits, 1) == data[1]).item() # 정확도 산출을 위하여 정답 개수 누적

print('Test Accuracy : %.2f%%'%(correct * 100 / len(X_test)))

X_pred = torch.tensor(np.array([train_images_contra[0], test_images_contra[0]]))
X2_pred = torch.tensor(np.array([train_images_contra[0], test_images_contra[0]]))

X_pred = X_pred.reshape(-1, 1, 28, 28)
X2_pred = X2_pred.reshape(-1, 1, 28, 28)

print(X_pred.shape)
print(X2_pred.shape)

pred_dataset = TensorDataset(X_pred, X2_pred)

#pred_dataset = TensorDataset(X_pred, X2_pred)
model.eval()

for data in tqdm(pred_dataset):

    logits = classifier(data[0])
    result = torch.argmax(logits, 1) # 정확도 산출을 위하여 정답 개수 누적
    print('result : ', result)

"""# K-nn : Origin"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

x_train, x_valid, y_train, y_valid = train_test_split(train_images, train_labels, stratify=train_labels, random_state=42, test_size=0.2)

x_train = x_train.reshape(-1, 28*28)
x_valid = x_valid.reshape(-1, 28*28)

print(x_train.shape)
print(y_train.shape)
print(x_valid.shape)
print(y_valid.shape)

knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
knn.fit(x_train, y_train)

y_valid_pred=knn.predict(x_valid)

valid_accuracy = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy:", valid_accuracy)

x_test = test_images.reshape(-1, 28*28)
y_test_pred = knn.predict(x_test)

test_accuracy = accuracy_score(test_labels, y_test_pred)
print("Test Accuracy:", test_accuracy)

k = 5
test_row = x_test[0]  # 예시로 첫 번째 테스트 데이터 사용
neighbors = knn.kneighbors([test_row], n_neighbors=k, return_distance=False)
cluster_labels = y_train[neighbors].flatten()

cluster_images = []
for i in neighbors[0]:
  cluster_images.append(x_train[i])
cluster_images = np.array(cluster_images)

print("Test Data Cluster Labels:", cluster_labels)

show_images(1, test[0], False)
show_images(len(cluster_images), [cluster_images])

"""# K-nn : with latent"""

train_images_latent = get_latent([train_images])

train_images_latent.shape

test_images_latent = get_latent([test_images])

test_images_latent.shape

x_train, x_valid, y_train, y_valid = train_test_split(train_images_latent, train_labels, stratify=train_labels, random_state=42, test_size=0.2)

print(x_train.shape)
print(y_train.shape)
print(x_valid.shape)
print(y_valid.shape)

x_train = x_train.reshape(-1, 7*7*8)
x_valid = x_valid.reshape(-1, 7*7*8)

knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
knn.fit(x_train, y_train)

y_valid_pred = knn.predict(x_valid)

valid_accuracy = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy:", np.round(valid_accuracy, 4))

x_test = test_images_latent.reshape(-1, 7*7*8)
y_test_pred = knn.predict(x_test)

test_accuracy = accuracy_score(test_labels, y_test_pred)
print("Test Accuracy:", np.round(test_accuracy, 4))

decoder = get_decoder()

x_train = x_train.reshape(-1, 7, 7, 8)
decoded_train_images = decoder.predict(x_train)

k = 5
test_row = x_test[0]  # 예시로 첫 번째 테스트 데이터 사용
neighbors = knn.kneighbors([test_row], n_neighbors=k, return_distance=False)
cluster_labels = y_train[neighbors].flatten()

cluster_images = []
for i in neighbors[0]:
  cluster_images.append(decoded_train_images[i])
cluster_images = np.array(cluster_images)

print("Test Data Cluster Labels:", cluster_labels)

show_images(1, test[0], False)
show_images(len(cluster_images), [cluster_images])

"""# Gan - 폰트 생성"""

from __future__ import print_function
from __future__ import absolute_import
import numpy as np
import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import DataLoader, TensorDataset
import torchvision.utils as vutils




def batch_norm(c_out, momentum=0.1):
    return nn.BatchNorm2d(c_out, momentum=momentum)


def conv2d(c_in, c_out, k_size=3, stride=2, pad=1, dilation=1, bn=True, lrelu=True, leak=0.2):
    layers = []
    if lrelu:
        layers.append(nn.LeakyReLU(leak))
    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad))
    if bn:
        layers.append(nn.BatchNorm2d(c_out))
    return nn.Sequential(*layers)


def deconv2d(c_in, c_out, k_size=3, stride=1, pad=1, dilation=1, bn=True, dropout=False, p=0.5):
    layers = []
    layers.append(nn.LeakyReLU(0.2))
    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad))
    if bn:
        layers.append(nn.BatchNorm2d(c_out))
    if dropout:
        layers.append(nn.Dropout(p))
    return nn.Sequential(*layers)


def lrelu(leak=0.2):
    return nn.LeakyReLU(leak)


def dropout(p=0.2):
    return nn.Dropout(p)


def fc(input_size, output_size):
    return nn.Linear(input_size, output_size)
    
    
def init_embedding(embedding_num, embedding_dim, stddev=0.01):
    embedding = torch.randn(embedding_num, embedding_dim) * stddev
    embedding = embedding.reshape((embedding_num, 1, 1, embedding_dim))
    return embedding


def embedding_lookup(embeddings, embedding_ids, GPU=False):
    batch_size = len(embedding_ids)
    embedding_dim = embeddings.shape[3]
    local_embeddings = []
    for id_ in embedding_ids:
        if GPU:
            local_embeddings.append(embeddings[id_].cpu().numpy())
        else:
            local_embeddings.append(embeddings[id_].data.numpy())
    local_embeddings = torch.from_numpy(np.array(local_embeddings))
    if GPU:
        local_embeddings = local_embeddings.cuda()
    local_embeddings = local_embeddings.reshape(batch_size, embedding_dim, 1, 1)
    return local_embeddings


def interpolated_embedding_lookup(embeddings, interpolated_embedding_ids, grid):
    batch_size = len(interpolated_embedding_ids)
    interpolated_embeddings = []
    embedding_dim = embeddings.shape[3]

    for id_ in interpolated_embedding_ids:
        interpolated_embeddings.append((embeddings[id_[0]] * (1 - grid) + embeddings[id_[1]] * grid).cpu().numpy())
    interpolated_embeddings = torch.from_numpy(np.array(interpolated_embeddings)).cuda()
    interpolated_embeddings = interpolated_embeddings.reshape(batch_size, embedding_dim, 1, 1)
    return interpolated_embeddings

class Encoder(nn.Module):
    
    def __init__(self, img_dim=1, conv_dim=64):
        super(Encoder, self).__init__()
        self.conv1 = conv2d(img_dim, conv_dim, k_size=5, stride=2, pad=2, dilation=2, lrelu=False, bn=False)
        self.conv2 = conv2d(conv_dim, conv_dim*2, k_size=5, stride=2, pad=2, dilation=2)
        self.conv3 = conv2d(conv_dim*2, conv_dim*4, k_size=4, stride=2, pad=1, dilation=1)
        self.conv4 = conv2d(conv_dim*4, conv_dim*8)
        self.conv5 = conv2d(conv_dim*8, conv_dim*8)
        self.conv6 = conv2d(conv_dim*8, conv_dim*8)
        self.conv7 = conv2d(conv_dim*8, conv_dim*8)
        self.conv8 = conv2d(conv_dim*8, conv_dim*8)
    
    def forward(self, images):
        encode_layers = dict()
        
        e1 = self.conv1(images)
        encode_layers['e1'] = e1
        e2 = self.conv2(e1)
        encode_layers['e2'] = e2
        e3 = self.conv3(e2)
        encode_layers['e3'] = e3
        e4 = self.conv4(e3)
        encode_layers['e4'] = e4
        e5 = self.conv5(e4)
        encode_layers['e5'] = e5
        e6 = self.conv6(e5)
        encode_layers['e6'] = e6
        e7 = self.conv7(e6)
        encode_layers['e7'] = e7
        encoded_source = self.conv8(e7)
        encode_layers['e8'] = encoded_source
        
        return encoded_source, encode_layers

class Decoder(nn.Module):
    
    def __init__(self, img_dim=1, embedded_dim=640, conv_dim=64):
        super(Decoder, self).__init__()
        self.deconv1 = deconv2d(embedded_dim, conv_dim*8, dropout=True)
        self.deconv2 = deconv2d(conv_dim*16, conv_dim*8, dropout=True, k_size=4)
        self.deconv3 = deconv2d(conv_dim*16, conv_dim*8, k_size=5, dilation=2, dropout=True)
        self.deconv4 = deconv2d(conv_dim*16, conv_dim*8, k_size=4, dilation=2, stride=2)
        self.deconv5 = deconv2d(conv_dim*16, conv_dim*4, k_size=4, dilation=2, stride=2)
        self.deconv6 = deconv2d(conv_dim*8, conv_dim*2, k_size=4, dilation=2, stride=2)
        self.deconv7 = deconv2d(conv_dim*4, conv_dim*1, k_size=4, dilation=2, stride=2)
        self.deconv8 = deconv2d(conv_dim*2, img_dim, k_size=4, dilation=2, stride=2, bn=False)
    
    
    def forward(self, embedded, encode_layers):
        
        d1 = self.deconv1(embedded)
        d1 = torch.cat((d1, encode_layers['e7']), dim=1)
        d2 = self.deconv2(d1)
        d2 = torch.cat((d2, encode_layers['e6']), dim=1)
        d3 = self.deconv3(d2)
        d3 = torch.cat((d3, encode_layers['e5']), dim=1)
        d4 = self.deconv4(d3)
        d4 = torch.cat((d4, encode_layers['e4']), dim=1)
        d5 = self.deconv5(d4)
        d5 = torch.cat((d5, encode_layers['e3']), dim=1)
        d6 = self.deconv6(d5)
        d6 = torch.cat((d6, encode_layers['e2']), dim=1)
        d7 = self.deconv7(d6)
        d7 = torch.cat((d7, encode_layers['e1']), dim=1)
        d8 = self.deconv8(d7)        
        fake_target = torch.tanh(d8)
        
        return fake_target

def Generator(images, En, De, embeddings, embedding_ids, GPU=False, encode_layers=False):
    encoded_source, encode_layers = En(images)
    local_embeddings = embedding_lookup(embeddings, embedding_ids, GPU=GPU)
    if GPU:
        encoded_source = encoded_source.cuda()
        local_embeddings = local_embeddings.cuda()
    embedded = torch.cat((encoded_source, local_embeddings), 1)
    fake_target = De(embedded, encode_layers)
    if encode_layers:
        return fake_target, encoded_source, encode_layers
    else:
        return fake_target, encoded_source

class Discriminator(nn.Module):
    def __init__(self, category_num, img_dim=2, disc_dim=64):
        super(Discriminator, self).__init__()
        self.conv1 = conv2d(img_dim, disc_dim, bn=False)
        self.conv2 = conv2d(disc_dim, disc_dim*2)
        self.conv3 = conv2d(disc_dim*2, disc_dim*4)
        self.conv4 = conv2d(disc_dim*4, disc_dim*8)
        self.fc1 = fc(disc_dim*8*8*8, 1)
        self.fc2 = fc(disc_dim*8*8*8, category_num)
        
    def forward(self, images):
        batch_size = images.shape[0]
        h1 = self.conv1(images)
        h2 = self.conv2(h1)
        h3 = self.conv3(h2)
        h4 = self.conv4(h3)
        
        tf_loss_logit = self.fc1(h4.reshape(batch_size, -1))
        tf_loss = torch.sigmoid(tf_loss_logit)
        cat_loss = self.fc2(h4.reshape(batch_size, -1))
        
        return tf_loss, tf_loss_logit, cat_loss

"""# GAN - 처음부터(Sequential)



"""

#!pip install -q imageio

import glob
import imageio
import matplotlib.pyplot as plt
import PIL
from tensorflow.keras import layers
import time
from IPython import display

(train_images, train_labels), (_, _) = mnist.load_data()

train_gan_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')
train_gan_images = (train_gan_images - 127.5) / 127.5 #이미지 [-1. 1] 정규화
print(train_gan_images.shape)

BUFFER_SIZE = len(train_gan_images)
BATCH_SIZE = 256
train_dataset = tf.data.Dataset.from_tensor_slices(train_gan_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

def create_generator():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # 주목: 배치사이즈로 None이 주어집니다.

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    return model

generator = create_generator()

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)

plt.imshow(generated_image[0, :, :, 0], cmap='gray')

def create_discriminator():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                                     input_shape=[28, 28, 1]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation='sigmoid'))

    return model

discriminator = create_discriminator()

decision = discriminator(generated_image)
print(decision)

# 이 메서드는 크로스 엔트로피 손실함수 (cross entropy loss)를 계산하기 위해 헬퍼 (helper) 함수를 반환합니다.
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

checkpoint_dir = "/content/drive/MyDrive/Colab/training_checkpoint/training_GAN/cp.ckpt"
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

EPOCHS = 50
noise_dim = 100
num_examples_to_generate = 16

seed = tf.random.normal([num_examples_to_generate, noise_dim])

# 이 데코레이터는 함수를 "컴파일"합니다.
@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator(noise, training=True)

      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images, training=True)

      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

def generate_and_save_images(model, epoch, test_input):
  # `training`이 False로 맞춰진 것을 주목하세요.
  # 이렇게 하면 (배치정규화를 포함하여) 모든 층들이 추론 모드로 실행됩니다. 
  predictions = model(test_input, training=False)

  fig = plt.figure(figsize=(4,4))

  for i in range(predictions.shape[0]):
      plt.subplot(4, 4, i+1)
      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
      plt.axis('off')

  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
  plt.show()

def train(dataset, epochs):
  for epoch in range(epochs):
    start = time.time()

    for image_batch in dataset:
      train_step(image_batch)

    # GIF를 위한 이미지를 바로 생성합니다.
    display.clear_output(wait=True)
    generate_and_save_images(generator, epoch + 1, seed)

    # 15 에포크가 지날 때마다 모델을 저장합니다.
    if (epoch + 1) % 15 == 0:
      checkpoint.save(file_prefix = checkpoint_dir)
    
    # print (' 에포크 {} 에서 걸린 시간은 {} 초 입니다'.format(epoch +1, time.time()-start))
    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))

  # 마지막 에포크가 끝난 후 생성합니다.
  display.clear_output(wait=True)
  generate_and_save_images(generator, epochs, seed)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# train(train_dataset, EPOCHS)

#마지막 체크 포인트
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

# 에포크 숫자를 사용하여 하나의 이미지를 보여줍니다.
def display_image(epoch_no):
  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))

anim_file = 'dcgan.gif'

with imageio.get_writer(anim_file, mode='I') as writer:
  filenames = glob.glob('image*.png')
  filenames = sorted(filenames)
  last = -1
  for i,filename in enumerate(filenames):
    frame = 2*(i**0.5)
    if round(frame) > round(last):
      last = frame
    else:
      continue
    image = imageio.imread(filename)
    writer.append_data(image)
  image = imageio.imread(filename)
  writer.append_data(image)

import IPython
if IPython.version_info > (6,2,0,''):
  display.Image(filename=anim_file)
 

 

try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download(anim_file)